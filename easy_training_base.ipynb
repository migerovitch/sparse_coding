{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting baukit@ git+https://github.com/davidbau/baukit@e14a18a6ad6cf9e0d6a5dc7a97e671e393c01682 (from -r requirements.txt (line 11))\n",
      "  Cloning https://github.com/davidbau/baukit (to revision e14a18a6ad6cf9e0d6a5dc7a97e671e393c01682) to /tmp/pip-install-agw8douc/baukit_a8a5ac8f25d5454580554407d3496e9a\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/davidbau/baukit /tmp/pip-install-agw8douc/baukit_a8a5ac8f25d5454580554407d3496e9a\n",
      "  Running command git rev-parse -q --verify 'sha^e14a18a6ad6cf9e0d6a5dc7a97e671e393c01682'\n",
      "  Running command git fetch -q https://github.com/davidbau/baukit e14a18a6ad6cf9e0d6a5dc7a97e671e393c01682\n",
      "  Running command git checkout -q e14a18a6ad6cf9e0d6a5dc7a97e671e393c01682\n",
      "  Resolved https://github.com/davidbau/baukit to commit e14a18a6ad6cf9e0d6a5dc7a97e671e393c01682\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting neuron-explainer@ git+https://github.com/openai/automated-interpretability.git@8be455788f43a603381e3c1b38a697ad4797a90f#subdirectory=neuron-explainer (from -r requirements.txt (line 85))\n",
      "  Cloning https://github.com/openai/automated-interpretability.git (to revision 8be455788f43a603381e3c1b38a697ad4797a90f) to /tmp/pip-install-agw8douc/neuron-explainer_838890e408344a91b2a11be69b5cd794\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/automated-interpretability.git /tmp/pip-install-agw8douc/neuron-explainer_838890e408344a91b2a11be69b5cd794\n",
      "  Running command git rev-parse -q --verify 'sha^8be455788f43a603381e3c1b38a697ad4797a90f'\n",
      "  Running command git fetch -q https://github.com/openai/automated-interpretability.git 8be455788f43a603381e3c1b38a697ad4797a90f\n",
      "  Running command git checkout -q 8be455788f43a603381e3c1b38a697ad4797a90f\n",
      "  Resolved https://github.com/openai/automated-interpretability.git to commit 8be455788f43a603381e3c1b38a697ad4797a90f\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformer-lens@ git+https://github.com/neelnanda-io/TransformerLens@ae32fa54ad40cb2c3f3a60f1837d0b4899c8daae (from -r requirements.txt (line 164))\n",
      "  Cloning https://github.com/neelnanda-io/TransformerLens (to revision ae32fa54ad40cb2c3f3a60f1837d0b4899c8daae) to /tmp/pip-install-agw8douc/transformer-lens_811877b2b3c34294ae0e6e14df7ccb6c\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens /tmp/pip-install-agw8douc/transformer-lens_811877b2b3c34294ae0e6e14df7ccb6c\n",
      "  Running command git rev-parse -q --verify 'sha^ae32fa54ad40cb2c3f3a60f1837d0b4899c8daae'\n",
      "  Running command git fetch -q https://github.com/neelnanda-io/TransformerLens ae32fa54ad40cb2c3f3a60f1837d0b4899c8daae\n",
      "  Running command git checkout -q ae32fa54ad40cb2c3f3a60f1837d0b4899c8daae\n",
      "  Resolved https://github.com/neelnanda-io/TransformerLens to commit ae32fa54ad40cb2c3f3a60f1837d0b4899c8daae\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting aiohttp==3.8.5 (from -r requirements.txt (line 1))\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiosignal==1.3.1 (from -r requirements.txt (line 2))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting anyio==3.7.0 (from -r requirements.txt (line 3))\n",
      "  Downloading anyio-3.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting appdirs==1.4.4 (from -r requirements.txt (line 4))\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting asttokens==2.4.0 (from -r requirements.txt (line 5))\n",
      "  Downloading asttokens-2.4.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting async-timeout==4.0.2 (from -r requirements.txt (line 6))\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: attrs==23.1.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (23.1.0)\n",
      "Collecting autobahn==23.6.1 (from -r requirements.txt (line 8))\n",
      "  Downloading autobahn-23.6.1.tar.gz (476 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.5/476.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting Automat==22.10.0 (from -r requirements.txt (line 9))\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: backcall==0.2.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.2.0)\n",
      "Collecting black==23.3.0 (from -r requirements.txt (line 12))\n",
      "  Downloading black-23.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting blobfile==2.0.2 (from -r requirements.txt (line 13))\n",
      "  Downloading blobfile-2.0.2-py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting blosc2==2.0.0 (from -r requirements.txt (line 14))\n",
      "  Downloading blosc2-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting boostedblob==0.13.2 (from -r requirements.txt (line 15))\n",
      "  Downloading boostedblob-0.13.2-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting boto3==1.26.158 (from -r requirements.txt (line 16))\n",
      "  Downloading boto3-1.26.158-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting botocore==1.29.158 (from -r requirements.txt (line 17))\n",
      "  Downloading botocore-1.29.158-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: certifi==2023.7.22 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (2023.7.22)\n",
      "Requirement already satisfied: cffi==1.15.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (1.15.1)\n",
      "Collecting charset-normalizer==3.1.0 (from -r requirements.txt (line 20))\n",
      "  Downloading charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting circuitsvis==1.41.0 (from -r requirements.txt (line 21))\n",
      "  Downloading circuitsvis-1.41.0-py3-none-any.whl.metadata (995 bytes)\n",
      "Collecting click==8.1.3 (from -r requirements.txt (line 22))\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cmake==3.26.4 (from -r requirements.txt (line 23))\n",
      "  Downloading cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting comm==0.1.4 (from -r requirements.txt (line 24))\n",
      "  Downloading comm-0.1.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting constantly==15.1.0 (from -r requirements.txt (line 25))\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting contourpy==1.0.7 (from -r requirements.txt (line 26))\n",
      "  Downloading contourpy-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography==41.0.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (41.0.3)\n",
      "Collecting cycler==0.11.0 (from -r requirements.txt (line 28))\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting Cython==3.0.2 (from -r requirements.txt (line 29))\n",
      "  Downloading Cython-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
      "Collecting datasets==2.12.0 (from -r requirements.txt (line 30))\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting debugpy==1.6.7.post1 (from -r requirements.txt (line 31))\n",
      "  Downloading debugpy-1.6.7.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 32)) (5.1.1)\n",
      "Collecting dill==0.3.6 (from -r requirements.txt (line 33))\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds==0.4.0 (from -r requirements.txt (line 34))\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting docstring-parser==0.15 (from -r requirements.txt (line 35))\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting einops==0.6.1 (from -r requirements.txt (line 36))\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting exceptiongroup==1.1.1 (from -r requirements.txt (line 37))\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Collecting executing==1.2.0 (from -r requirements.txt (line 38))\n",
      "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting fancy-einsum==0.0.3 (from -r requirements.txt (line 39))\n",
      "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
      "Collecting filelock==3.12.0 (from -r requirements.txt (line 40))\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting fonttools==4.39.4 (from -r requirements.txt (line 41))\n",
      "  Downloading fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist==1.3.3 (from -r requirements.txt (line 42))\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec==2023.5.0 (from -r requirements.txt (line 43))\n",
      "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitdb==4.0.10 (from -r requirements.txt (line 44))\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting GitPython==3.1.32 (from -r requirements.txt (line 45))\n",
      "  Downloading GitPython-3.1.32-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting graphviz==0.20.1 (from -r requirements.txt (line 46))\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11==0.14.0 (from -r requirements.txt (line 47))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hkdf==0.0.3 (from -r requirements.txt (line 48))\n",
      "  Downloading hkdf-0.0.3.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httpcore==0.17.2 (from -r requirements.txt (line 49))\n",
      "  Downloading httpcore-0.17.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting httpx==0.24.1 (from -r requirements.txt (line 50))\n",
      "  Downloading httpx-0.24.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting huggingface-hub==0.15.1 (from -r requirements.txt (line 51))\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting humanize==4.6.0 (from -r requirements.txt (line 52))\n",
      "  Downloading humanize-4.6.0-py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.0/110.0 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hyperlink==21.0.0 (from -r requirements.txt (line 53))\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna==3.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 54)) (3.4)\n",
      "Collecting importlib-metadata==5.2.0 (from -r requirements.txt (line 55))\n",
      "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Collecting importlib-resources==5.12.0 (from -r requirements.txt (line 56))\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting incremental==22.10.0 (from -r requirements.txt (line 57))\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: iniconfig==2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 58)) (2.0.0)\n",
      "Collecting ipykernel==6.25.2 (from -r requirements.txt (line 59))\n",
      "  Downloading ipykernel-6.25.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: ipython==8.15.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 60)) (8.15.0)\n",
      "Collecting jaxtyping==0.2.19 (from -r requirements.txt (line 61))\n",
      "  Downloading jaxtyping-0.2.19-py3-none-any.whl (24 kB)\n",
      "Collecting jedi==0.19.0 (from -r requirements.txt (line 62))\n",
      "  Downloading jedi-0.19.0-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: Jinja2==3.1.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 63)) (3.1.2)\n",
      "Collecting jmespath==1.0.1 (from -r requirements.txt (line 64))\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting joblib==1.2.0 (from -r requirements.txt (line 65))\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jupyter_client==8.3.1 (from -r requirements.txt (line 66))\n",
      "  Downloading jupyter_client-8.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyter_core==5.3.1 (from -r requirements.txt (line 67))\n",
      "  Downloading jupyter_core-5.3.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting kiwisolver==1.4.4 (from -r requirements.txt (line 68))\n",
      "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lit==16.0.6 (from -r requirements.txt (line 69))\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lxml==4.9.2 (from -r requirements.txt (line 70))\n",
      "  Downloading lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting magic-wormhole==0.12.0 (from -r requirements.txt (line 71))\n",
      "  Downloading magic_wormhole-0.12.0-py2.py3-none-any.whl (201 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py==2.2.0 (from -r requirements.txt (line 72))\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe==2.1.2 (from -r requirements.txt (line 73))\n",
      "  Downloading MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting matplotlib==3.7.1 (from -r requirements.txt (line 74))\n",
      "  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib-inline==0.1.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 75)) (0.1.6)\n",
      "Collecting mdurl==0.1.2 (from -r requirements.txt (line 76))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 77)) (1.3.0)\n",
      "Collecting msgpack==1.0.5 (from -r requirements.txt (line 78))\n",
      "  Downloading msgpack-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multidict==6.0.4 (from -r requirements.txt (line 79))\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess==0.70.14 (from -r requirements.txt (line 80))\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mypy==1.3.0 (from -r requirements.txt (line 81))\n",
      "  Downloading mypy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mypy-extensions==1.0.0 (from -r requirements.txt (line 82))\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting nest-asyncio==1.5.7 (from -r requirements.txt (line 83))\n",
      "  Downloading nest_asyncio-1.5.7-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: networkx==3.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 84)) (3.1)\n",
      "Collecting numexpr==2.8.5 (from -r requirements.txt (line 86))\n",
      "  Downloading numexpr-2.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting numpy==1.24.3 (from -r requirements.txt (line 87))\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from -r requirements.txt (line 88))\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from -r requirements.txt (line 89))\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from -r requirements.txt (line 90))\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from -r requirements.txt (line 91))\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from -r requirements.txt (line 92))\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from -r requirements.txt (line 93))\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from -r requirements.txt (line 94))\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from -r requirements.txt (line 95))\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from -r requirements.txt (line 96))\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from -r requirements.txt (line 97))\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from -r requirements.txt (line 98))\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting optree==0.9.1 (from -r requirements.txt (line 99))\n",
      "  Downloading optree-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson==3.9.1 (from -r requirements.txt (line 100))\n",
      "  Downloading orjson-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging==23.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 101)) (23.1)\n",
      "Collecting pandas==2.0.2 (from -r requirements.txt (line 102))\n",
      "  Downloading pandas-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: parso==0.8.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 103)) (0.8.3)\n",
      "Collecting pathspec==0.11.1 (from -r requirements.txt (line 104))\n",
      "  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n",
      "Collecting pathtools==0.1.2 (from -r requirements.txt (line 105))\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pexpect==4.8.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 106)) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 107)) (0.7.5)\n",
      "Collecting Pillow==9.5.0 (from -r requirements.txt (line 108))\n",
      "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting platformdirs==3.5.3 (from -r requirements.txt (line 109))\n",
      "  Downloading platformdirs-3.5.3-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting plotly==5.14.1 (from -r requirements.txt (line 110))\n",
      "  Downloading plotly-5.14.1-py2.py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pluggy==1.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 111)) (1.0.0)\n",
      "Collecting progressbar2==4.2.0 (from -r requirements.txt (line 112))\n",
      "  Downloading progressbar2-4.2.0-py2.py3-none-any.whl (27 kB)\n",
      "Collecting prompt-toolkit==3.0.39 (from -r requirements.txt (line 113))\n",
      "  Downloading prompt_toolkit-3.0.39-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting protobuf==4.23.2 (from -r requirements.txt (line 114))\n",
      "  Downloading protobuf-4.23.2-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Collecting psutil==5.9.5 (from -r requirements.txt (line 115))\n",
      "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ptyprocess==0.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 116)) (0.7.0)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 117)) (0.2.2)\n",
      "Collecting py-cpuinfo==9.0.0 (from -r requirements.txt (line 118))\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting pyarrow==12.0.0 (from -r requirements.txt (line 119))\n",
      "  Downloading pyarrow-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.9/38.9 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1==0.5.0 (from -r requirements.txt (line 120))\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1-modules==0.3.0 (from -r requirements.txt (line 121))\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycparser==2.21 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 122)) (2.21)\n",
      "Collecting pycryptodomex==3.18.0 (from -r requirements.txt (line 123))\n",
      "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: Pygments==2.15.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 124)) (2.15.1)\n",
      "Collecting PyNaCl==1.5.0 (from -r requirements.txt (line 125))\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyOpenSSL==23.2.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 126)) (23.2.0)\n",
      "Collecting pyparsing==3.0.9 (from -r requirements.txt (line 127))\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytest==7.3.2 (from -r requirements.txt (line 128))\n",
      "  Downloading pytest-7.3.2-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 129)) (2.8.2)\n",
      "Collecting python-utils==3.7.0 (from -r requirements.txt (line 130))\n",
      "  Downloading python_utils-3.7.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting pytz==2023.3 (from -r requirements.txt (line 131))\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyYAML==6.0 (from -r requirements.txt (line 132))\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyzmq==25.1.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 133)) (25.1.1)\n",
      "Collecting regex==2023.5.5 (from -r requirements.txt (line 134))\n",
      "  Downloading regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests==2.31.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 135)) (2.31.0)\n",
      "Collecting responses==0.18.0 (from -r requirements.txt (line 136))\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting rich==13.4.1 (from -r requirements.txt (line 137))\n",
      "  Downloading rich-13.4.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting s3transfer==0.6.1 (from -r requirements.txt (line 138))\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors==0.3.1 (from -r requirements.txt (line 139))\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn==1.2.2 (from -r requirements.txt (line 140))\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy==1.10.1 (from -r requirements.txt (line 141))\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentry-sdk==1.24.0 (from -r requirements.txt (line 142))\n",
      "  Downloading sentry_sdk-1.24.0-py2.py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting service-identity==21.1.0 (from -r requirements.txt (line 143))\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting setproctitle==1.3.2 (from -r requirements.txt (line 144))\n",
      "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: six==1.16.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 145)) (1.16.0)\n",
      "Collecting smmap==5.0.0 (from -r requirements.txt (line 146))\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: sniffio==1.3.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 147)) (1.3.0)\n",
      "Collecting spake2==0.8 (from -r requirements.txt (line 148))\n",
      "  Downloading spake2-0.8-py2.py3-none-any.whl (39 kB)\n",
      "Collecting stack-data==0.6.2 (from -r requirements.txt (line 149))\n",
      "  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
      "Collecting sympy==1.12 (from -r requirements.txt (line 150))\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tables==3.8.0 (from -r requirements.txt (line 151))\n",
      "  Downloading tables-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tenacity==8.2.2 (from -r requirements.txt (line 152))\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Collecting threadpoolctl==3.1.0 (from -r requirements.txt (line 153))\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting tiktoken==0.4.0 (from -r requirements.txt (line 154))\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers==0.13.3 (from -r requirements.txt (line 155))\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tomli==2.0.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 156)) (2.0.1)\n",
      "Collecting torch==2.0.1 (from -r requirements.txt (line 157))\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchopt==0.7.1 (from -r requirements.txt (line 158))\n",
      "  Downloading torchopt-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchtyping==0.1.4 (from -r requirements.txt (line 159))\n",
      "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
      "Collecting torchvision==0.15.2 (from -r requirements.txt (line 160))\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado==6.3.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 161)) (6.3.3)\n",
      "Requirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 162)) (4.65.0)\n",
      "Collecting traitlets==5.9.0 (from -r requirements.txt (line 163))\n",
      "  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.30.1 (from -r requirements.txt (line 165))\n",
      "  Downloading transformers-4.30.1-py3-none-any.whl.metadata (113 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0 (from -r requirements.txt (line 166))\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting Twisted==22.10.0 (from -r requirements.txt (line 167))\n",
      "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting txaio==23.1.1 (from -r requirements.txt (line 168))\n",
      "  Downloading txaio-23.1.1-py2.py3-none-any.whl (30 kB)\n",
      "Collecting txtorcon==23.5.0 (from -r requirements.txt (line 169))\n",
      "  Downloading txtorcon-23.5.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting typed-argument-parser==1.8.0 (from -r requirements.txt (line 170))\n",
      "  Downloading typed-argument-parser-1.8.0.tar.gz (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typeguard==3.0.2 (from -r requirements.txt (line 171))\n",
      "  Downloading typeguard-3.0.2-py3-none-any.whl (30 kB)\n",
      "Collecting types-requests==2.31.0.1 (from -r requirements.txt (line 172))\n",
      "  Downloading types_requests-2.31.0.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting types-urllib3==1.26.25.13 (from -r requirements.txt (line 173))\n",
      "  Downloading types_urllib3-1.26.25.13-py3-none-any.whl (15 kB)\n",
      "Collecting typing-inspect==0.9.0 (from -r requirements.txt (line 174))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting typing_extensions==4.6.2 (from -r requirements.txt (line 175))\n",
      "  Downloading typing_extensions-4.6.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting tzdata==2023.3 (from -r requirements.txt (line 176))\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3==1.26.16 (from -r requirements.txt (line 177))\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvloop==0.17.0 (from -r requirements.txt (line 178))\n",
      "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wandb==0.15.3 (from -r requirements.txt (line 179))\n",
      "  Downloading wandb-0.15.3-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting wcwidth==0.2.6 (from -r requirements.txt (line 180))\n",
      "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
      "Collecting xxhash==3.2.0 (from -r requirements.txt (line 181))\n",
      "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl==1.9.2 (from -r requirements.txt (line 182))\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zipp==3.15.0 (from -r requirements.txt (line 183))\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting zope.interface==6.0 (from -r requirements.txt (line 184))\n",
      "  Downloading zope.interface-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from autobahn==23.6.1->-r requirements.txt (line 8)) (68.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->-r requirements.txt (line 88)) (0.41.2)\n",
      "Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.0-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading asttokens-2.4.0-py2.py3-none-any.whl (27 kB)\n",
      "Downloading boto3-1.26.158-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.29.158-py3-none-any.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading circuitsvis-1.41.0-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading comm-0.1.4-py3-none-any.whl (6.6 kB)\n",
      "Downloading Cython-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading debugpy-1.6.7.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ipykernel-6.25.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.2/154.2 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_client-8.3.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_core-5.3.1-py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB)\n",
      "Downloading numexpr-2.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading optree-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (314 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.5/314.5 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-3.5.3-py3-none-any.whl (15 kB)\n",
      "Downloading prompt_toolkit-3.0.39-py3-none-any.whl (385 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.2/385.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.23.2-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytest-7.3.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.9/320.9 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_utils-3.7.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading rich-13.4.1-py3-none-any.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-1.24.0-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.5/206.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchopt-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (672 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m672.0/672.0 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading txtorcon-23.5.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.31.0.1-py3-none-any.whl (14 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_extensions-4.6.2-py3-none-any.whl (31 kB)\n",
      "Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: autobahn, hkdf, lit, pathtools, typed-argument-parser, baukit, neuron-explainer, transformer-lens\n",
      "  Building wheel for autobahn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autobahn: filename=autobahn-23.6.1-cp310-cp310-linux_x86_64.whl size=661570 sha256=38b297a52553a005bad98e571eeb6aa628c0b223a6c010f858c4bd1ea216f646\n",
      "  Stored in directory: /root/.cache/pip/wheels/f6/f4/25/1bcb043ba95af19d969e81814ed9a5dd7784604dc77ba668e8\n",
      "  Building wheel for hkdf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hkdf: filename=hkdf-0.0.3-py3-none-any.whl size=3716 sha256=36276ab06d32699def09d13a2f1f7834a2b02b53b8aa5c1a320b54770c5a9805\n",
      "  Stored in directory: /root/.cache/pip/wheels/6c/f9/c4/b665eda8a25167cf86e4550c69497c877c8e7e56ec664f8a0a\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=00fe4209013a3f6a0711e22fd5eb74c370f4793cb505f1e916448ef6b71a49fc\n",
      "  Stored in directory: /root/.cache/pip/wheels/14/f9/07/bb2308587bc2f57158f905a2325f6a89a2befa7437b2d7e137\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=2cb8fe3f9dd6d364c7a713e5592003b1366731b1a385e477f2b4cabaeb041221\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "  Building wheel for typed-argument-parser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for typed-argument-parser: filename=typed_argument_parser-1.8.0-py3-none-any.whl size=25452 sha256=dd98533c903281d217a9088c75e3c7129e99ec80778c25fd4beafe4b66711127\n",
      "  Stored in directory: /root/.cache/pip/wheels/ed/39/c5/b470e80aba24fdd11df9c33de12094085a8cb8b30ee10f970a\n",
      "  Building wheel for baukit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for baukit: filename=baukit-0.0.1-py3-none-any.whl size=59669 sha256=cb5d32b5aa1143498a7eb04e320af55d4d1abe98021b39ffad44249095cf59d9\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/c4/20/0fd3492db50c0d7ae5a2aa22ee51810eef4a5d7c76dd6dd381\n",
      "  Building wheel for neuron-explainer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for neuron-explainer: filename=neuron_explainer-0.0.1-py3-none-any.whl size=41618 sha256=f7895b9c4545be810dec4db0befcd3365955ae053d1ee1332b3feb6009fd3781\n",
      "  Stored in directory: /root/.cache/pip/wheels/1b/73/02/4e8c6dc42df0708ecdc8c4f9bfa2941f9e9d22807707a688e8\n",
      "  Building wheel for transformer-lens (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformer-lens: filename=transformer_lens-0.0.0-py3-none-any.whl size=99732 sha256=868fa7a541b3f599bc1c719ae1c236d3b0ce0ec3a976ac7c11ec7a4312cece07\n",
      "  Stored in directory: /root/.cache/pip/wheels/bb/46/20/4fb47943261bf710809df613de9e6d8f7e924cb6f28be34658\n",
      "Successfully built autobahn hkdf lit pathtools typed-argument-parser baukit neuron-explainer transformer-lens\n",
      "Installing collected packages: wcwidth, types-urllib3, tokenizers, safetensors, pytz, py-cpuinfo, pathtools, msgpack, lit, incremental, hkdf, executing, constantly, cmake, appdirs, zope.interface, zipp, xxhash, uvloop, urllib3, tzdata, typing_extensions, types-requests, txaio, traitlets, threadpoolctl, tenacity, sympy, spake2, smmap, setproctitle, regex, PyYAML, pyparsing, pycryptodomex, pyasn1, psutil, protobuf, prompt-toolkit, platformdirs, Pillow, pathspec, orjson, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, nest-asyncio, mypy-extensions, multidict, mdurl, MarkupSafe, lxml, kiwisolver, joblib, jmespath, jedi, importlib-resources, hyperlink, humanize, h11, graphviz, fsspec, frozenlist, fonttools, filelock, fancy-einsum, exceptiongroup, einops, docstring-parser, docker-pycreds, dill, debugpy, Cython, cycler, click, charset-normalizer, blosc2, Automat, async-timeout, asttokens, yarl, typing-inspect, typeguard, Twisted, stack-data, sentry-sdk, scipy, python-utils, pytest, PyNaCl, pyasn1-modules, pyarrow, plotly, pandas, optree, nvidia-cusolver-cu11, nvidia-cudnn-cu11, numexpr, mypy, multiprocess, markdown-it-py, jupyter_core, importlib-metadata, gitdb, contourpy, comm, botocore, blobfile, black, anyio, aiosignal, typed-argument-parser, tiktoken, tables, service-identity, scikit-learn, s3transfer, rich, responses, progressbar2, matplotlib, jupyter_client, jaxtyping, huggingface-hub, httpcore, GitPython, autobahn, aiohttp, wandb, transformers, ipykernel, httpx, boto3, boostedblob, txtorcon, neuron-explainer, datasets, magic-wormhole, triton, torch, torchvision, transformer-lens, torchtyping, torchopt, circuitsvis, baukit\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.5\n",
      "    Uninstalling wcwidth-0.2.5:\n",
      "      Successfully uninstalled wcwidth-0.2.5\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3.post1\n",
      "    Uninstalling pytz-2023.3.post1:\n",
      "      Successfully uninstalled pytz-2023.3.post1\n",
      "  Attempting uninstall: executing\n",
      "    Found existing installation: executing 0.8.3\n",
      "    Uninstalling executing-0.8.3:\n",
      "      Successfully uninstalled executing-0.8.3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.7.1\n",
      "    Uninstalling traitlets-5.7.1:\n",
      "      Successfully uninstalled traitlets-5.7.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.11.1\n",
      "    Uninstalling sympy-1.11.1:\n",
      "      Successfully uninstalled sympy-1.11.1\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.0\n",
      "    Uninstalling psutil-5.9.0:\n",
      "      Successfully uninstalled psutil-5.9.0\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt-toolkit 3.0.36\n",
      "    Uninstalling prompt-toolkit-3.0.36:\n",
      "      Successfully uninstalled prompt-toolkit-3.0.36\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 4.0.0\n",
      "    Uninstalling platformdirs-4.0.0:\n",
      "      Successfully uninstalled platformdirs-4.0.0\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 10.0.1\n",
      "    Uninstalling Pillow-10.0.1:\n",
      "      Successfully uninstalled Pillow-10.0.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.5.8\n",
      "    Uninstalling nest-asyncio-1.5.8:\n",
      "      Successfully uninstalled nest-asyncio-1.5.8\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.1\n",
      "    Uninstalling MarkupSafe-2.1.1:\n",
      "      Successfully uninstalled MarkupSafe-2.1.1\n",
      "  Attempting uninstall: jedi\n",
      "    Found existing installation: jedi 0.18.1\n",
      "    Uninstalling jedi-0.18.1:\n",
      "      Successfully uninstalled jedi-0.18.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.10.0\n",
      "    Uninstalling fsspec-2023.10.0:\n",
      "      Successfully uninstalled fsspec-2023.10.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.9.0\n",
      "    Uninstalling filelock-3.9.0:\n",
      "      Successfully uninstalled filelock-3.9.0\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.0.4\n",
      "    Uninstalling exceptiongroup-1.0.4:\n",
      "      Successfully uninstalled exceptiongroup-1.0.4\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.8.0\n",
      "    Uninstalling debugpy-1.8.0:\n",
      "      Successfully uninstalled debugpy-1.8.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.0.4\n",
      "    Uninstalling charset-normalizer-2.0.4:\n",
      "      Successfully uninstalled charset-normalizer-2.0.4\n",
      "  Attempting uninstall: asttokens\n",
      "    Found existing installation: asttokens 2.0.5\n",
      "    Uninstalling asttokens-2.0.5:\n",
      "      Successfully uninstalled asttokens-2.0.5\n",
      "  Attempting uninstall: stack-data\n",
      "    Found existing installation: stack-data 0.2.0\n",
      "    Uninstalling stack-data-0.2.0:\n",
      "      Successfully uninstalled stack-data-0.2.0\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 7.4.3\n",
      "    Uninstalling pytest-7.4.3:\n",
      "      Successfully uninstalled pytest-7.4.3\n",
      "  Attempting uninstall: jupyter_core\n",
      "    Found existing installation: jupyter_core 5.5.0\n",
      "    Uninstalling jupyter_core-5.5.0:\n",
      "      Successfully uninstalled jupyter_core-5.5.0\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.2.0\n",
      "    Uninstalling comm-0.2.0:\n",
      "      Successfully uninstalled comm-0.2.0\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "  Attempting uninstall: jupyter_client\n",
      "    Found existing installation: jupyter_client 8.6.0\n",
      "    Uninstalling jupyter_client-8.6.0:\n",
      "      Successfully uninstalled jupyter_client-8.6.0\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.26.0\n",
      "    Uninstalling ipykernel-6.26.0:\n",
      "      Successfully uninstalled ipykernel-6.26.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.1\n",
      "    Uninstalling torch-2.1.1:\n",
      "      Successfully uninstalled torch-2.1.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.1\n",
      "    Uninstalling torchvision-0.16.1:\n",
      "      Successfully uninstalled torchvision-0.16.1\n",
      "Successfully installed Automat-22.10.0 Cython-3.0.2 GitPython-3.1.32 MarkupSafe-2.1.2 Pillow-9.5.0 PyNaCl-1.5.0 PyYAML-6.0 Twisted-22.10.0 aiohttp-3.8.5 aiosignal-1.3.1 anyio-3.7.0 appdirs-1.4.4 asttokens-2.4.0 async-timeout-4.0.2 autobahn-23.6.1 baukit-0.0.1 black-23.3.0 blobfile-2.0.2 blosc2-2.0.0 boostedblob-0.13.2 boto3-1.26.158 botocore-1.29.158 charset-normalizer-3.1.0 circuitsvis-1.41.0 click-8.1.3 cmake-3.26.4 comm-0.1.4 constantly-15.1.0 contourpy-1.0.7 cycler-0.11.0 datasets-2.12.0 debugpy-1.6.7.post1 dill-0.3.6 docker-pycreds-0.4.0 docstring-parser-0.15 einops-0.6.1 exceptiongroup-1.1.1 executing-1.2.0 fancy-einsum-0.0.3 filelock-3.12.0 fonttools-4.39.4 frozenlist-1.3.3 fsspec-2023.5.0 gitdb-4.0.10 graphviz-0.20.1 h11-0.14.0 hkdf-0.0.3 httpcore-0.17.2 httpx-0.24.1 huggingface-hub-0.15.1 humanize-4.6.0 hyperlink-21.0.0 importlib-metadata-5.2.0 importlib-resources-5.12.0 incremental-22.10.0 ipykernel-6.25.2 jaxtyping-0.2.19 jedi-0.19.0 jmespath-1.0.1 joblib-1.2.0 jupyter_client-8.3.1 jupyter_core-5.3.1 kiwisolver-1.4.4 lit-16.0.6 lxml-4.9.2 magic-wormhole-0.12.0 markdown-it-py-2.2.0 matplotlib-3.7.1 mdurl-0.1.2 msgpack-1.0.5 multidict-6.0.4 multiprocess-0.70.14 mypy-1.3.0 mypy-extensions-1.0.0 nest-asyncio-1.5.7 neuron-explainer-0.0.1 numexpr-2.8.5 numpy-1.24.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 optree-0.9.1 orjson-3.9.1 pandas-2.0.2 pathspec-0.11.1 pathtools-0.1.2 platformdirs-3.5.3 plotly-5.14.1 progressbar2-4.2.0 prompt-toolkit-3.0.39 protobuf-4.23.2 psutil-5.9.5 py-cpuinfo-9.0.0 pyarrow-12.0.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycryptodomex-3.18.0 pyparsing-3.0.9 pytest-7.3.2 python-utils-3.7.0 pytz-2023.3 regex-2023.5.5 responses-0.18.0 rich-13.4.1 s3transfer-0.6.1 safetensors-0.3.1 scikit-learn-1.2.2 scipy-1.10.1 sentry-sdk-1.24.0 service-identity-21.1.0 setproctitle-1.3.2 smmap-5.0.0 spake2-0.8 stack-data-0.6.2 sympy-1.12 tables-3.8.0 tenacity-8.2.2 threadpoolctl-3.1.0 tiktoken-0.4.0 tokenizers-0.13.3 torch-2.0.1 torchopt-0.7.1 torchtyping-0.1.4 torchvision-0.15.2 traitlets-5.9.0 transformer-lens-0.0.0 transformers-4.30.1 triton-2.0.0 txaio-23.1.1 txtorcon-23.5.0 typed-argument-parser-1.8.0 typeguard-3.0.2 types-requests-2.31.0.1 types-urllib3-1.26.25.13 typing-inspect-0.9.0 typing_extensions-4.6.2 tzdata-2023.3 urllib3-1.26.16 uvloop-0.17.0 wandb-0.15.3 wcwidth-0.2.6 xxhash-3.2.0 yarl-1.9.2 zipp-3.15.0 zope.interface-6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import argparse\n",
    "from utils import dotdict\n",
    "from activation_dataset import setup_token_data\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def init_cnf():\n",
    "    cfg = dotdict()\n",
    "    # models: \"EleutherAI/pythia-6.9b\", \"lomahony/eleuther-pythia6.9b-hh-sft\", \"usvsnsp/pythia-6.9b-ppo\", \"Dahoas/gptj-rm-static\", \"reciprocate/dahoas-gptj-rm-static\"\n",
    "    # cfg.model_name=\"lomahony/eleuther-pythia6.9b-hh-sft\"\n",
    "    # \"EleutherAI/pythia-70m\", \"lomahony/pythia-70m-helpful-sft\", \"lomahony/eleuther-pythia70m-hh-sft\"\n",
    "    cfg.model_name=\"EleutherAI/pythia-70m-deduped\"\n",
    "    cfg.layers=[0,] # Change this to run multiple layers\n",
    "    cfg.setting=\"residual\"\n",
    "    # cfg.tensor_name=\"gpt_neox.layers.{layer}\" or \"transformer.h.{layer}\"\n",
    "    cfg.tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "    original_l1_alpha = 8e-4\n",
    "    cfg.l1_alpha=original_l1_alpha\n",
    "    cfg.l1_alphas=[8e-5, 1e-4, 2e-4, 4e-4, 8e-4, 1e-3, 2e-3, 4e-3, 8e-3]\n",
    "    cfg.sparsity=None\n",
    "    cfg.num_epochs=10\n",
    "    cfg.model_batch_size=8\n",
    "    cfg.lr=1e-3 # ORIGINAL: 1e-3\n",
    "    cfg.kl=False\n",
    "    cfg.reconstruction=False\n",
    "    #cfg.dataset_name=\"NeelNanda/pile-10k\"\n",
    "    cfg.dataset_name=\"Elriggs/openwebtext-100k\"\n",
    "    cfg.device=\"cuda:0\"\n",
    "    cfg.ratio = 4\n",
    "    cfg.seed = 0\n",
    "    # cfg.device=\"cpu\"\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = [cfg.tensor_name.format(layer=layer) for layer in cfg.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, GPTJForSequenceClassification\n",
    "\n",
    "\n",
    "# Load in the model\n",
    "def load_model(cfg):\n",
    "    model = AutoModelForCausalLM.from_pretrained(cfg.model_name)\n",
    "    model = model.to(cfg.device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-8401ec7d4dbd84d2_*_of_00008.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 112750592\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "# TODO iteratively grab dataset?\n",
    "def init_dataloader(cfg, model, tokenizer):\n",
    "    cfg.max_length = 256\n",
    "    token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed)\n",
    "    num_tokens = cfg.max_length*cfg.model_batch_size*len(token_loader)\n",
    "    print(f\"Number of tokens: {num_tokens}\")\n",
    "    return token_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation size: 512\n"
     ]
    }
   ],
   "source": [
    "# Run 1 datapoint on model to get the activation size\n",
    "from baukit import Trace\n",
    "\n",
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# Your activation name will be different. In the next cells, we will show you how to find it.\n",
    "with torch.no_grad():\n",
    "    with Trace(model, tensor_names[0]) as ret:\n",
    "        _ = model(tokens)\n",
    "        representation = ret.output\n",
    "        # check if instance tuple\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "        activation_size = representation.shape[-1]\n",
    "print(f\"Activation size: {activation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sparsity: 25\n"
     ]
    }
   ],
   "source": [
    "# Set target sparsity to 10% of activation_size if not set\n",
    "\n",
    "# NOT USED\n",
    "if cfg.sparsity is None:\n",
    "    cfg.sparsity = int(activation_size*0.05)\n",
    "    print(f\"Target sparsity: {cfg.sparsity}\")\n",
    "\n",
    "target_lower_sparsity = cfg.sparsity * 0.9\n",
    "target_upper_sparsity = cfg.sparsity * 1.1\n",
    "adjustment_factor = 0.1  # You can set this to whatever you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Autoencocer init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_autoencoder(cfg):\n",
    "    params = dict()\n",
    "    n_dict_components = activation_size*cfg.ratio # Sparse Autoencoder Size\n",
    "    params[\"encoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "    nn.init.xavier_uniform_(params[\"encoder\"])\n",
    "\n",
    "    params[\"decoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "    nn.init.xavier_uniform_(params[\"decoder\"])\n",
    "\n",
    "    params[\"encoder_bias\"] = torch.empty((n_dict_components,), device=cfg.device)\n",
    "    nn.init.zeros_(params[\"encoder_bias\"])\n",
    "\n",
    "    params[\"shift_bias\"] = torch.empty((activation_size,), device=cfg.device)\n",
    "    nn.init.zeros_(params[\"shift_bias\"])\n",
    "\n",
    "    autoencoder = AnthropicSAE(  # TiedSAE, UntiedSAE, AnthropicSAE\n",
    "        # n_feats = n_dict_components, \n",
    "        # activation_size=activation_size,\n",
    "        encoder=params[\"encoder\"],\n",
    "        encoder_bias=params[\"encoder_bias\"],\n",
    "        decoder=params[\"decoder\"],\n",
    "        shift_bias=params[\"shift_bias\"],\n",
    "    )\n",
    "    autoencoder.to_device(cfg.device)\n",
    "    autoencoder.set_grad()\n",
    "    l1_variants.append(autoencoder)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            autoencoder.encoder, \n",
    "            autoencoder.encoder_bias,\n",
    "            autoencoder.decoder,\n",
    "            autoencoder.shift_bias,\n",
    "        ], lr=cfg.lr)\n",
    "    l1_optimizers.append(optimizer)\n",
    "    return autoencoder, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize New autoencoder\n",
    "from autoencoders.learned_dict import AnthropicSAE\n",
    "from torch import nn\n",
    "autoencoders = []\n",
    "optimizers = []\n",
    "for layer in cfg.layers:\n",
    "    l1_variants = []\n",
    "    l1_optimizers = []\n",
    "    for l1 in cfg.l1_alphas:\n",
    "        params = dict()\n",
    "        n_dict_components = activation_size*cfg.ratio # Sparse Autoencoder Size\n",
    "        params[\"encoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "        nn.init.xavier_uniform_(params[\"encoder\"])\n",
    "\n",
    "        params[\"decoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "        nn.init.xavier_uniform_(params[\"decoder\"])\n",
    "\n",
    "        params[\"encoder_bias\"] = torch.empty((n_dict_components,), device=cfg.device)\n",
    "        nn.init.zeros_(params[\"encoder_bias\"])\n",
    "\n",
    "        params[\"shift_bias\"] = torch.empty((activation_size,), device=cfg.device)\n",
    "        nn.init.zeros_(params[\"shift_bias\"])\n",
    "\n",
    "        autoencoder = AnthropicSAE(  # TiedSAE, UntiedSAE, AnthropicSAE\n",
    "            # n_feats = n_dict_components, \n",
    "            # activation_size=activation_size,\n",
    "            encoder=params[\"encoder\"],\n",
    "            encoder_bias=params[\"encoder_bias\"],\n",
    "            decoder=params[\"decoder\"],\n",
    "            shift_bias=params[\"shift_bias\"],\n",
    "        )\n",
    "        autoencoder.to_device(cfg.device)\n",
    "        autoencoder.set_grad()\n",
    "        l1_variants.append(autoencoder)\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                autoencoder.encoder, \n",
    "                autoencoder.encoder_bias,\n",
    "                autoencoder.decoder,\n",
    "                autoencoder.shift_bias,\n",
    "            ], lr=cfg.lr)\n",
    "        l1_optimizers.append(optimizer)\n",
    "        \n",
    "    autoencoders.append(l1_variants)\n",
    "    optimizers.append(l1_optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_run_name: EleutherAI/pythia-70m-deduped_1210-203835_lr0.001\n"
     ]
    }
   ],
   "source": [
    "# original_bias = autoencoder.encoder_bias.clone().detach()\n",
    "# Wandb setup\n",
    "def setup_wandb(cfg):\n",
    "    secrets = json.load(open(\"secrets.json\"))\n",
    "    wandb.login(key=secrets[\"wandb_key\"])\n",
    "    start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    wandb_run_name = f\"{cfg.model_name}_{start_time[4:]}_lr{cfg.lr}\"  # trim year\n",
    "    wandb.init(project=\"sparse coding tests\", config=dict(cfg), name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55054it [15:49, 58.01it/s]                           \n"
     ]
    }
   ],
   "source": [
    "def training_run(cfg, model, optimizer, autoencoder):\n",
    "\n",
    "    time_since_activation = torch.zeros(autoencoder.encoder.shape[0])\n",
    "    total_activations = torch.zeros(autoencoder.encoder.shape[0])\n",
    "    \n",
    "    max_num_tokens = 100_000_000\n",
    "    save_every = 30_000\n",
    "    num_saved_so_far = 0\n",
    "    \n",
    "    # Freeze model parameters \n",
    "    model.eval()\n",
    "    model.requires_grad_(False)\n",
    "    model.to(cfg.device)\n",
    "    \n",
    "    last_encoder = autoencoder.encoder.clone().detach()\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(token_loader,total=int(max_num_tokens/(cfg.max_length*cfg.model_batch_size)))):\n",
    "        tokens = batch[\"input_ids\"].to(cfg.device)\n",
    "    \n",
    "        # print(f\"tokens shape: {tokens.shape}\")\n",
    "        \n",
    "        with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "            \n",
    "            #print(tensor_names)\n",
    "            \n",
    "            with Trace(model, tensor_names[0]) as ret:\n",
    "                _ = model(tokens)\n",
    "                representation = ret.output\n",
    "                \n",
    "                if(isinstance(representation, tuple)):\n",
    "                    representation = representation[0]\n",
    "        #print(f\"representation is: {representation}\")\n",
    "        #print(f\"representation shape is: {representation.shape}\")\n",
    "        layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "    \n",
    "        # activation_saver.save_batch(layer_activations.clone().cpu().detach())\n",
    "        \n",
    "        c = autoencoder.encode(layer_activations)\n",
    "        x_hat = autoencoder.decode(c)\n",
    "        \n",
    "        reconstruction_loss = (x_hat - layer_activations).pow(2).mean()\n",
    "        l1_loss = torch.norm(c, 1, dim=-1).mean()\n",
    "        total_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "    \n",
    "        time_since_activation += 1\n",
    "        time_since_activation = time_since_activation * (c.sum(dim=0).cpu()==0)\n",
    "        # total_activations += c.sum(dim=0).cpu()\n",
    "        if ((i) % 100 == 0): # Check here so first check is model w/o change\n",
    "            # self_similarity = torch.cosine_similarity(c, last_encoder, dim=-1).mean().cpu().item()\n",
    "            # Above is wrong, should be similarity between encoder and last encoder\n",
    "            self_similarity = torch.cosine_similarity(autoencoder.encoder, last_encoder, dim=-1).mean().cpu().item()\n",
    "            last_encoder = autoencoder.encoder.clone().detach()\n",
    "    \n",
    "            num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "            with torch.no_grad():\n",
    "                sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "                # Count number of dead_features are zero\n",
    "                num_dead_features = (time_since_activation >= min(i, 200)).sum().item()\n",
    "            #print(f\"Sparsity: {sparsity:.1f} | Dead Features: {num_dead_features} | Total Loss: {total_loss:.2f} | Reconstruction Loss: {reconstruction_loss:.2f} | L1 Loss: {cfg.l1_alpha*l1_loss:.2f} | l1_alpha: {cfg.l1_alpha:.2e} | Tokens: {num_tokens_so_far} | Self Similarity: {self_similarity:.2f}\")\n",
    "            \n",
    "            wandb.log({\n",
    "                'Sparsity': sparsity,\n",
    "                'Dead Features': num_dead_features,\n",
    "                'Total Loss': total_loss.item(),\n",
    "                'Reconstruction Loss': reconstruction_loss.item(),\n",
    "                'L1 Loss': (cfg.l1_alpha*l1_loss).item(),\n",
    "                'l1_alpha': cfg.l1_alpha,\n",
    "                'Tokens': num_tokens_so_far,\n",
    "                'Self Similarity': self_similarity\n",
    "            })\n",
    "            \n",
    "            dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "            \n",
    "            # if(num_tokens_so_far > max_num_tokens):\n",
    "            #     print(f\"Reached max number of tokens: {max_num_tokens}\")\n",
    "            #     break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resample_period = 10000\n",
    "        # if (i % resample_period == 0):\n",
    "        #     # RESAMPLING\n",
    "        #     with torch.no_grad():\n",
    "        #         # Count number of dead_features are zero\n",
    "        #         num_dead_features = (total_activations == 0).sum().item()\n",
    "        #         print(f\"Dead Features: {num_dead_features}\")\n",
    "                \n",
    "        #     if num_dead_features > 0:\n",
    "        #         print(\"Resampling!\")\n",
    "        #         # hyperparams:\n",
    "        #         max_resample_tokens = 1000 # the number of token activations that we consider for inserting into the dictionary\n",
    "        #         # compute loss of model on random subset of inputs\n",
    "        #         resample_loader = setup_token_data(cfg, tokenizer, model, seed=i)\n",
    "        #         num_resample_data = 0\n",
    "    \n",
    "        #         resample_activations = torch.empty(0, activation_size)\n",
    "        #         resample_losses = torch.empty(0)\n",
    "    \n",
    "        #         for resample_batch in resample_loader:\n",
    "        #             resample_tokens = resample_batch[\"input_ids\"].to(cfg.device)\n",
    "        #             with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "        #                 with Trace(model, tensor_names[0]) as ret:\n",
    "        #                     _ = model(resample_tokens)\n",
    "        #                     representation = ret.output\n",
    "        #                     if(isinstance(representation, tuple)):\n",
    "        #                         representation = representation[0]\n",
    "        #             layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "        #             resample_activations = torch.cat((resample_activations, layer_activations.detach().cpu()), dim=0)\n",
    "    \n",
    "        #             c = autoencoder.encode(layer_activations)\n",
    "        #             x_hat = autoencoder.decode(c)\n",
    "                    \n",
    "        #             reconstruction_loss = (x_hat - layer_activations).pow(2).mean(dim=-1)\n",
    "        #             l1_loss = torch.norm(c, 1, dim=-1)\n",
    "        #             temp_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "                    \n",
    "        #             resample_losses = torch.cat((resample_losses, temp_loss.detach().cpu()), dim=0)\n",
    "                    \n",
    "        #             num_resample_data +=layer_activations.shape[0]\n",
    "        #             if num_resample_data > max_resample_tokens:\n",
    "        #                 break\n",
    "    \n",
    "                    \n",
    "        #         # sample num_dead_features vectors of input activations\n",
    "        #         probabilities = resample_losses**2\n",
    "        #         probabilities /= probabilities.sum()\n",
    "        #         sampled_indices = torch.multinomial(probabilities, num_dead_features, replacement=True)\n",
    "        #         new_vectors = resample_activations[sampled_indices]\n",
    "    \n",
    "        #         # calculate average encoder norm of alive neurons\n",
    "        #         alive_neurons = list((total_activations!=0))\n",
    "        #         modified_columns = total_activations==0\n",
    "        #         avg_norm = autoencoder.encoder.data[alive_neurons].norm(dim=-1).mean()\n",
    "    \n",
    "        #         # replace dictionary and encoder weights with vectors\n",
    "        #         new_vectors = new_vectors / new_vectors.norm(dim=1, keepdim=True)\n",
    "                \n",
    "        #         params_to_modify = [autoencoder.encoder, autoencoder.encoder_bias]\n",
    "    \n",
    "        #         current_weights = autoencoder.encoder.data\n",
    "        #         current_weights[modified_columns] = (new_vectors.to(cfg.device) * avg_norm * 0.02)\n",
    "        #         autoencoder.encoder.data = current_weights\n",
    "    \n",
    "        #         current_weights = autoencoder.encoder_bias.data\n",
    "        #         current_weights[modified_columns] = 0\n",
    "        #         autoencoder.encoder_bias.data = current_weights\n",
    "                \n",
    "        #         if hasattr(autoencoder, 'decoder'):\n",
    "        #             current_weights = autoencoder.decoder.data\n",
    "        #             current_weights[modified_columns] = new_vectors.to(cfg.device)\n",
    "        #             autoencoder.decoder.data = current_weights\n",
    "        #             params_to_modify += [autoencoder.decoder]\n",
    "    \n",
    "        #         for param_group in optimizer.param_groups:\n",
    "        #             for param in param_group['params']:\n",
    "        #                 if any(param is d_ for d_ in params_to_modify):\n",
    "        #                     # Extract the corresponding rows from m and v\n",
    "        #                     m = optimizer.state[param]['exp_avg']\n",
    "        #                     v = optimizer.state[param]['exp_avg_sq']\n",
    "                            \n",
    "        #                     # Update the m and v values for the modified columns\n",
    "        #                     m[modified_columns] = 0  # Reset moving average for modified columns\n",
    "        #                     v[modified_columns] = 0  # Reset squared moving average for modified columns\n",
    "            \n",
    "        #     total_activations = torch.zeros(autoencoder.encoder.shape[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        # if ((i+2) % save_every ==0): # save periodically but before big changes\n",
    "        #     model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "        #     save_name = f\"{model_save_name}_sp{cfg.sparsity}_r{cfg.ratio}_{tensor_names[0]}_ckpt{num_saved_so_far}\"  # trim year\n",
    "    \n",
    "        #     # Make directory traiend_models if it doesn't exist\n",
    "        #     import os\n",
    "        #     if not os.path.exists(\"trained_models\"):\n",
    "        #         os.makedirs(\"trained_models\")\n",
    "        #     # Save model\n",
    "        #     torch.save(autoencoder, f\"trained_models/{save_name}.pt\")\n",
    "            \n",
    "        #     num_saved_so_far += 1\n",
    "    \n",
    "        # # Running sparsity check\n",
    "        # num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "        # if(num_tokens_so_far > 200000):\n",
    "        #     if(i % 100 == 0):\n",
    "        #         with torch.no_grad():\n",
    "        #             sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "        #         if sparsity > target_upper_sparsity:\n",
    "        #             cfg.l1_alpha *= (1 + adjustment_factor)\n",
    "        #         elif sparsity < target_lower_sparsity:\n",
    "        #             cfg.l1_alpha *= (1 - adjustment_factor)\n",
    "        #         # print(f\"Sparsity: {sparsity:.1f} | l1_alpha: {cfg.l1_alpha:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Duplicated training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55054it [15:49, 58.01it/s]                           \n"
     ]
    }
   ],
   "source": [
    "time_since_activation = torch.zeros(autoencoder.encoder.shape[0])\n",
    "total_activations = torch.zeros(autoencoder.encoder.shape[0])\n",
    "\n",
    "max_num_tokens = 100_000_000\n",
    "save_every = 30_000\n",
    "num_saved_so_far = 0\n",
    "\n",
    "# Freeze model parameters \n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model.to(cfg.device)\n",
    "\n",
    "last_encoder = autoencoder.encoder.clone().detach()\n",
    "\n",
    "for i, batch in enumerate(tqdm(token_loader,total=int(max_num_tokens/(cfg.max_length*cfg.model_batch_size)))):\n",
    "    tokens = batch[\"input_ids\"].to(cfg.device)\n",
    "\n",
    "    # print(f\"tokens shape: {tokens.shape}\")\n",
    "    \n",
    "    with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "        \n",
    "        #print(tensor_names)\n",
    "        \n",
    "        with Trace(model, tensor_names[0]) as ret:\n",
    "            _ = model(tokens)\n",
    "            representation = ret.output\n",
    "            \n",
    "            if(isinstance(representation, tuple)):\n",
    "                representation = representation[0]\n",
    "    #print(f\"representation is: {representation}\")\n",
    "    #print(f\"representation shape is: {representation.shape}\")\n",
    "    layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "\n",
    "    # activation_saver.save_batch(layer_activations.clone().cpu().detach())\n",
    "    for layer in range(len(cfg.layers)):\n",
    "        for l1_alpha in range(1): #len(cfg.l1_alphas)):\n",
    "            autoencoder = autoencoders[layer][l1_alpha]\n",
    "            optimizer = optimizers[layer][l1_alpha]\n",
    "            \n",
    "            c = autoencoder.encode(layer_activations)\n",
    "            x_hat = autoencoder.decode(c)\n",
    "            \n",
    "            reconstruction_loss = (x_hat - layer_activations).pow(2).mean()\n",
    "            l1_loss = torch.norm(c, 1, dim=-1).mean()\n",
    "            total_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "\n",
    "            time_since_activation += 1\n",
    "            time_since_activation = time_since_activation * (c.sum(dim=0).cpu()==0)\n",
    "            # total_activations += c.sum(dim=0).cpu()\n",
    "            if ((i) % 100 == 0): # Check here so first check is model w/o change\n",
    "                # self_similarity = torch.cosine_similarity(c, last_encoder, dim=-1).mean().cpu().item()\n",
    "                # Above is wrong, should be similarity between encoder and last encoder\n",
    "                self_similarity = torch.cosine_similarity(autoencoder.encoder, last_encoder, dim=-1).mean().cpu().item()\n",
    "                last_encoder = autoencoder.encoder.clone().detach()\n",
    "\n",
    "                num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "                with torch.no_grad():\n",
    "                    sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "                    # Count number of dead_features are zero\n",
    "                    num_dead_features = (time_since_activation >= min(i, 200)).sum().item()\n",
    "                #print(f\"Sparsity: {sparsity:.1f} | Dead Features: {num_dead_features} | Total Loss: {total_loss:.2f} | Reconstruction Loss: {reconstruction_loss:.2f} | L1 Loss: {cfg.l1_alpha*l1_loss:.2f} | l1_alpha: {cfg.l1_alpha:.2e} | Tokens: {num_tokens_so_far} | Self Similarity: {self_similarity:.2f}\")\n",
    "                \n",
    "                wandb.log({\n",
    "                    'Sparsity': sparsity,\n",
    "                    'Dead Features': num_dead_features,\n",
    "                    'Total Loss': total_loss.item(),\n",
    "                    'Reconstruction Loss': reconstruction_loss.item(),\n",
    "                    'L1 Loss': (cfg.l1_alpha*l1_loss).item(),\n",
    "                    'l1_alpha': cfg.l1_alpha,\n",
    "                    'Tokens': num_tokens_so_far,\n",
    "                    'Self Similarity': self_similarity\n",
    "                })\n",
    "                \n",
    "                dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "                \n",
    "                # if(num_tokens_so_far > max_num_tokens):\n",
    "                #     print(f\"Reached max number of tokens: {max_num_tokens}\")\n",
    "                #     break\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # resample_period = 10000\n",
    "    # if (i % resample_period == 0):\n",
    "    #     # RESAMPLING\n",
    "    #     with torch.no_grad():\n",
    "    #         # Count number of dead_features are zero\n",
    "    #         num_dead_features = (total_activations == 0).sum().item()\n",
    "    #         print(f\"Dead Features: {num_dead_features}\")\n",
    "            \n",
    "    #     if num_dead_features > 0:\n",
    "    #         print(\"Resampling!\")\n",
    "    #         # hyperparams:\n",
    "    #         max_resample_tokens = 1000 # the number of token activations that we consider for inserting into the dictionary\n",
    "    #         # compute loss of model on random subset of inputs\n",
    "    #         resample_loader = setup_token_data(cfg, tokenizer, model, seed=i)\n",
    "    #         num_resample_data = 0\n",
    "\n",
    "    #         resample_activations = torch.empty(0, activation_size)\n",
    "    #         resample_losses = torch.empty(0)\n",
    "\n",
    "    #         for resample_batch in resample_loader:\n",
    "    #             resample_tokens = resample_batch[\"input_ids\"].to(cfg.device)\n",
    "    #             with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "    #                 with Trace(model, tensor_names[0]) as ret:\n",
    "    #                     _ = model(resample_tokens)\n",
    "    #                     representation = ret.output\n",
    "    #                     if(isinstance(representation, tuple)):\n",
    "    #                         representation = representation[0]\n",
    "    #             layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "    #             resample_activations = torch.cat((resample_activations, layer_activations.detach().cpu()), dim=0)\n",
    "\n",
    "    #             c = autoencoder.encode(layer_activations)\n",
    "    #             x_hat = autoencoder.decode(c)\n",
    "                \n",
    "    #             reconstruction_loss = (x_hat - layer_activations).pow(2).mean(dim=-1)\n",
    "    #             l1_loss = torch.norm(c, 1, dim=-1)\n",
    "    #             temp_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "                \n",
    "    #             resample_losses = torch.cat((resample_losses, temp_loss.detach().cpu()), dim=0)\n",
    "                \n",
    "    #             num_resample_data +=layer_activations.shape[0]\n",
    "    #             if num_resample_data > max_resample_tokens:\n",
    "    #                 break\n",
    "\n",
    "                \n",
    "    #         # sample num_dead_features vectors of input activations\n",
    "    #         probabilities = resample_losses**2\n",
    "    #         probabilities /= probabilities.sum()\n",
    "    #         sampled_indices = torch.multinomial(probabilities, num_dead_features, replacement=True)\n",
    "    #         new_vectors = resample_activations[sampled_indices]\n",
    "\n",
    "    #         # calculate average encoder norm of alive neurons\n",
    "    #         alive_neurons = list((total_activations!=0))\n",
    "    #         modified_columns = total_activations==0\n",
    "    #         avg_norm = autoencoder.encoder.data[alive_neurons].norm(dim=-1).mean()\n",
    "\n",
    "    #         # replace dictionary and encoder weights with vectors\n",
    "    #         new_vectors = new_vectors / new_vectors.norm(dim=1, keepdim=True)\n",
    "            \n",
    "    #         params_to_modify = [autoencoder.encoder, autoencoder.encoder_bias]\n",
    "\n",
    "    #         current_weights = autoencoder.encoder.data\n",
    "    #         current_weights[modified_columns] = (new_vectors.to(cfg.device) * avg_norm * 0.02)\n",
    "    #         autoencoder.encoder.data = current_weights\n",
    "\n",
    "    #         current_weights = autoencoder.encoder_bias.data\n",
    "    #         current_weights[modified_columns] = 0\n",
    "    #         autoencoder.encoder_bias.data = current_weights\n",
    "            \n",
    "    #         if hasattr(autoencoder, 'decoder'):\n",
    "    #             current_weights = autoencoder.decoder.data\n",
    "    #             current_weights[modified_columns] = new_vectors.to(cfg.device)\n",
    "    #             autoencoder.decoder.data = current_weights\n",
    "    #             params_to_modify += [autoencoder.decoder]\n",
    "\n",
    "    #         for param_group in optimizer.param_groups:\n",
    "    #             for param in param_group['params']:\n",
    "    #                 if any(param is d_ for d_ in params_to_modify):\n",
    "    #                     # Extract the corresponding rows from m and v\n",
    "    #                     m = optimizer.state[param]['exp_avg']\n",
    "    #                     v = optimizer.state[param]['exp_avg_sq']\n",
    "                        \n",
    "    #                     # Update the m and v values for the modified columns\n",
    "    #                     m[modified_columns] = 0  # Reset moving average for modified columns\n",
    "    #                     v[modified_columns] = 0  # Reset squared moving average for modified columns\n",
    "        \n",
    "    #     total_activations = torch.zeros(autoencoder.encoder.shape[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # if ((i+2) % save_every ==0): # save periodically but before big changes\n",
    "    #     model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "    #     save_name = f\"{model_save_name}_sp{cfg.sparsity}_r{cfg.ratio}_{tensor_names[0]}_ckpt{num_saved_so_far}\"  # trim year\n",
    "\n",
    "    #     # Make directory traiend_models if it doesn't exist\n",
    "    #     import os\n",
    "    #     if not os.path.exists(\"trained_models\"):\n",
    "    #         os.makedirs(\"trained_models\")\n",
    "    #     # Save model\n",
    "    #     torch.save(autoencoder, f\"trained_models/{save_name}.pt\")\n",
    "        \n",
    "    #     num_saved_so_far += 1\n",
    "\n",
    "    # # Running sparsity check\n",
    "    # num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "    # if(num_tokens_so_far > 200000):\n",
    "    #     if(i % 100 == 0):\n",
    "    #         with torch.no_grad():\n",
    "    #             sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "    #         if sparsity > target_upper_sparsity:\n",
    "    #             cfg.l1_alpha *= (1 + adjustment_factor)\n",
    "    #         elif sparsity < target_lower_sparsity:\n",
    "    #             cfg.l1_alpha *= (1 - adjustment_factor)\n",
    "    #         # print(f\"Sparsity: {sparsity:.1f} | l1_alpha: {cfg.l1_alpha:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_save(cfg, model):\n",
    "    model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "\n",
    "    start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    save_name = f\"{model_save_name}_sp{cfg.sparsity}_r{cfg.ratio}_{tensor_names[0]}\"  # trim year\n",
    "    \n",
    "    # Make directory traiend_models if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists(\"trained_models\"):\n",
    "        os.makedirs(\"trained_models\")\n",
    "    # Save model\n",
    "    torch.save(autoencoder, f\"trained_models/{save_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0680866350754d99a64032afde44be6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dead Features</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>L1 Loss</td><td>█▇▆▄▃▃▁▃▂▂▄▂▅▃▆▂▃▄▂▂▂▁▄▂▃▃▁▁▄▂▃▂▄▃▄▂▂▂▂▂</td></tr><tr><td>Reconstruction Loss</td><td>▇▅▄▃▂▂▁▂▂▁▅▂▄▂█▂▃▄▂▁▂▁▅▃▃▃▁▁▄▂▃▂▅▂▄▂▁▂▂▃</td></tr><tr><td>Self Similarity</td><td>▃▆▅▅▄▇▇▇▇▇▇▇▇▇█▇▇██▇▇▇███▁█▇▇████▇▇██▇▆█</td></tr><tr><td>Sparsity</td><td>█▇▆▄▃▂▂▃▂▂▃▂▄▂▅▂▂▃▂▁▂▁▄▂▂▂▁▁▃▂▂▂▃▂▃▂▁▂▂▂</td></tr><tr><td>Tokens</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Total Loss</td><td>█▆▅▃▂▂▁▃▂▂▅▂▅▃█▂▃▄▂▁▂▁▅▃▃▃▁▁▄▂▃▂▅▂▄▂▁▂▂▃</td></tr><tr><td>l1_alpha</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dead Features</td><td>0</td></tr><tr><td>L1 Loss</td><td>0.00982</td></tr><tr><td>Reconstruction Loss</td><td>0.01157</td></tr><tr><td>Self Similarity</td><td>0.99507</td></tr><tr><td>Sparsity</td><td>41.36523</td></tr><tr><td>Tokens</td><td>112640000</td></tr><tr><td>Total Loss</td><td>0.02139</td></tr><tr><td>l1_alpha</td><td>0.0008</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">EleutherAI/pythia-70m-deduped_1210-203835_lr0.001</strong> at: <a href='https://wandb.ai/mgerov/sparse%20coding%20tests/runs/slm11tol' target=\"_blank\">https://wandb.ai/mgerov/sparse%20coding%20tests/runs/slm11tol</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231210_203835-slm11tol/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
